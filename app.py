import os
import sys

# --- 1. CORRECTIF POUR LE CLOUD (Obligatoire pour Linux/Streamlit Cloud) ---
try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass 

import streamlit as st
import google.generativeai as genai
import chromadb
import time

# --- 2. CONFIGURATION DE LA PAGE ---
st.set_page_config(page_title="Comprendre Ma Paie", page_icon="üí°", layout="centered")
st.title("Comprendre Ma Paie üí°")
st.caption("L'assistant expert pour d√©crypter votre bulletin de salaire ¬©2025 Sylvain Attal")

# --- 3. S√âCURIT√â & CONNEXION ---
with st.sidebar:
    st.header("üîê Configuration")
    api_key = None
    
    # Tentative de r√©cup√©ration automatique depuis les secrets
    try:
        if "GOOGLE_API_KEY" in st.secrets:
            api_key = st.secrets["GOOGLE_API_KEY"]
            st.success("‚úÖ Cl√© API connect√©e (Mode Illimit√©)")
    except:
        pass

    # Champ manuel si les secrets ne fonctionnent pas en local
    if not api_key:
        api_key = st.text_input("Entrez votre cl√© API Google", type="password")
    
    if api_key:
        genai.configure(api_key=api_key)

if not api_key:
    st.warning("‚¨ÖÔ∏è Veuillez configurer votre cl√© API pour commencer.")
    st.stop()

# --- 4. LE CERVEAU (Base de donn√©es vectorielle) ---
@st.cache_resource(show_spinner=False)
def charger_cerveau():
    client = chromadb.Client()
    nom_collection = "paie_expert_v5" # Nouvelle version pour forcer la lecture de tous les fichiers

    try:
        client.delete_collection(nom_collection)
    except:
        pass
    
    collection = client.create_collection(nom_collection)

    # R√©cup√©ration de TOUS les fichiers .txt (Taux + Explications)
    tous_les_fichiers = [f for f in os.listdir('.') if f.endswith('.txt') and f != 'requirements.txt']
    
    if not tous_les_fichiers:
        return None

    docs_globaux = []
    ids_globaux = []
    compteur = 0
    
    # Lecture et d√©coupage
    for fichier in tous_les_fichiers:
        with open(fichier, "r", encoding="utf-8") as f:
            contenu = f.read()
        
        taille_bloc = 1000
        chevauchement = 100
        
        for i in range(0, len(contenu), taille_bloc - chevauchement):
            morceau = contenu[i : i + taille_bloc]
            if len(morceau.strip()) > 10:
                docs_globaux.append(f"Source [{fichier}] : {morceau}")
                ids_globaux.append(f"doc_{compteur}")
                compteur += 1

    if not docs_globaux:
        return None

    # Vectorisation (Embedding)
    embeddings = []
    total = len(docs_globaux)
    barre = st.progress(0, text=f"Lecture des documents de r√©f√©rence ({total} extraits)...")
    
    # Mod√®le d'embedding (gratuit et performant)
    modele_embedding = "models/text-embedding-004"

    try:
        # Test rapide de connexion
        genai.embed_content(model=modele_embedding, content="Test", task_type="retrieval_document")
    except Exception as e:
        barre.empty()
        st.error(f"‚õîÔ∏è Erreur de connexion API : {e}")
        return None

    for i, doc in enumerate(docs_globaux):
        try:
            res = genai.embed_content(model=modele_embedding, content=doc, task_type="retrieval_document")
            embeddings.append(res['embedding'])
            time.sleep(0.05) # Tr√®s rapide car quota illimit√© maintenant
        except:
            pass
        barre.progress(min((i + 1) / total, 1.0))
    
    barre.empty()
    
    if len(embeddings) > 0:
        collection.add(documents=docs_globaux, ids=ids_globaux, embeddings=embeddings)
        return collection
    return None

# --- 5. INTERFACE DE CHAT ---
with st.spinner("Initialisation de l'expert..."):
    db = charger_cerveau()

if db:
    st.success("‚úÖ Assistant pr√™t √† r√©pondre !")
else:
    st.error("‚ùå Aucun document trouv√©. Veuillez v√©rifier la pr√©sence des fichiers .txt.")

# Historique de conversation
if "messages" not in st.session_state:
    # La phrase ci-dessous est bien sur une seule ligne pour √©viter le bug
    st.session_state.messages = [{"role": "assistant", "content": "Bonjour ! Je suis connect√© aux bar√®mes officiels 2025. Quelle ligne de votre bulletin de paie voulez-vous comprendre ?"}]

for msg in st.session_state.messages:
    # Avatar personnalis√© : Cravate pour l'assistant, Bonhomme pour l'utilisateur
    icone = "üëî" if msg["role"] == "assistant" else "üë§"
    st.chat_message(msg["role"], avatar=icone).write(msg["content"])

# Zone de saisie
if question := st.chat_input("Votre question (ex: C'est quoi la CSG ? Mon brut est de 3000‚Ç¨...)"):
    st.session_state.messages.append({"role": "user", "content": question})
    st.chat_message("user", avatar="üë§").write(question)

    if db:
        try:
            # 1. Recherche RAG
            q_vec = genai.embed_content(model="models/text-embedding-004", content=question, task_type="retrieval_query")
            res = db.query(query_embeddings=[q_vec['embedding']], n_results=5)
            
            if res['documents'] and res['documents'][0]:
                contexte = "\n\n".join(res['documents'][0])
                
                # 2. Prompt Expert & P√©dagogue
                prompt = f"""Tu es un Expert Paie et P√©dagogue.
                Ta mission : R√©pondre √† la question du salari√© en utilisant les bar√®mes officiels fournis ci-dessous.
                
                R√®gles d'or :
                - Ton : Bienveillant, clair, rassurant.
                - Pr√©cision : Utilise les chiffres du contexte (Taux 2025).
                - Si on te demande un calcul, fais-le √©tape par √©tape.
                - Cite tes sources implicitement ("Selon les bar√®mes officiels...").
                
                DOCUMENTS DE R√âF√âRENCE (CONTEXTE) :
                {contexte}
                
                QUESTION DU SALARI√â : {question}"""
                
                # --- LE MOTEUR (Maintenant d√©brid√© gr√¢ce √† la facturation) ---
                # On utilise le mod√®le 2.0 Flash standard
                model = genai.GenerativeModel('models/gemini-2.0-flash')
                
                reponse = model.generate_content(prompt)
                
                st.chat_message("assistant", avatar="üëî").write(reponse.text)
                st.session_state.messages.append({"role": "assistant", "content": reponse.text})
            else:
                st.warning("Je n'ai pas trouv√© cette information dans mes documents de r√©f√©rence.")
        except Exception as e:
            st.error(f"Une erreur technique est survenue : {e}")